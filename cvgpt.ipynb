{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 21:42:08.647264: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-05 21:42:09.462745: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-05 21:42:09.462836: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-05 21:42:09.462845: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import youtokentome as yttm\n",
    "from dotmap import DotMap\n",
    "\n",
    "from dnnutils.generate import BeamGenerator, ensure_length\n",
    "from dnnutils.nn_helper import init_random_seed, train_eval_loop, predict_with_model\n",
    "from dnnutils.tensorboard import SummaryWriterHelper\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [6, 4]\n",
    "tqdm.pandas()\n",
    "\n",
    "SEED = 42\n",
    "init_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evigurskiy/.venvs/sharedenv/lib/python3.10/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DotMap()\n",
    "config.MAX_INPUT_TEXT_SEQ_LEN = 512\n",
    "config.SEQ_SEPARATOR = '<SEP>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare dataset from file vremya_valeri\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16218/16218 [00:00<00:00, 55358.03it/s]\n",
      "100%|██████████| 5747/5747 [00:15<00:00, 370.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6099 sequences for file vremya_valeri\n",
      "Prepare dataset from file israel_repatriation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80482/80482 [00:01<00:00, 79079.26it/s]\n",
      "100%|██████████| 31181/31181 [02:02<00:00, 253.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 34174 sequences for file israel_repatriation\n",
      "Prepare dataset from file better_data_community\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6697/6697 [00:00<00:00, 49456.31it/s]\n",
      "100%|██████████| 3688/3688 [00:05<00:00, 665.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2286 sequences for file better_data_community\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Попробую добавить возможность комментировать</td>\n",
       "      <td>Не работает...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Попробую добавить возможность комментировать&lt;S...</td>\n",
       "      <td>Прекрасно, ждём</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data-driven team management</td>\n",
       "      <td>Это скорее для корпораций. А тут вопрос в прав...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Недавно я улетал из Лондона и прилетал в Москв...</td>\n",
       "      <td>Кажется что правильным было бы сравнивать два ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Недавно я улетал из Лондона и прилетал в Москв...</td>\n",
       "      <td>В таком случае и классы домов должны быть сопо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42554</th>\n",
       "      <td>Ещё есть страшные слова metric learning и trip...</td>\n",
       "      <td>Contrastive learning посмотри</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42555</th>\n",
       "      <td>Не нашел инфу об Eqvilent. Чем занимаетесь, ка...</td>\n",
       "      <td>вот же</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42556</th>\n",
       "      <td>у нас есть джун вакансии, можешь податься и пи...</td>\n",
       "      <td>С вами я уже в процессе)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42557</th>\n",
       "      <td>На дзене)))</td>\n",
       "      <td>А видео на рутубе</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42558</th>\n",
       "      <td>Пока был 1ый собес</td>\n",
       "      <td>найс</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42559 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       X  \\\n",
       "0           Попробую добавить возможность комментировать   \n",
       "1      Попробую добавить возможность комментировать<S...   \n",
       "2                            data-driven team management   \n",
       "3      Недавно я улетал из Лондона и прилетал в Москв...   \n",
       "4      Недавно я улетал из Лондона и прилетал в Москв...   \n",
       "...                                                  ...   \n",
       "42554  Ещё есть страшные слова metric learning и trip...   \n",
       "42555  Не нашел инфу об Eqvilent. Чем занимаетесь, ка...   \n",
       "42556  у нас есть джун вакансии, можешь податься и пи...   \n",
       "42557                                        На дзене)))   \n",
       "42558                                 Пока был 1ый собес   \n",
       "\n",
       "                                                       y  \n",
       "0                                         Не работает...  \n",
       "1                                        Прекрасно, ждём  \n",
       "2      Это скорее для корпораций. А тут вопрос в прав...  \n",
       "3      Кажется что правильным было бы сравнивать два ...  \n",
       "4      В таком случае и классы домов должны быть сопо...  \n",
       "...                                                  ...  \n",
       "42554                      Contrastive learning посмотри  \n",
       "42555                                             вот же  \n",
       "42556                           С вами я уже в процессе)  \n",
       "42557                                  А видео на рутубе  \n",
       "42558                                               найс  \n",
       "\n",
       "[42559 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dataset sequences\n",
    "\n",
    "df_dataset = pd.DataFrame(columns=['X', 'y'])\n",
    "\n",
    "def parse_message(row, messages):\n",
    "    row_idx = row.name\n",
    "    message = messages[row_idx]\n",
    "    row['message_id'] = message['id']\n",
    "    row['date'] = pd.to_datetime(message['date'])\n",
    "    if 'from' not in message:\n",
    "        return row\n",
    "    row['from'] = message['from']\n",
    "    row['from_id'] = message['from_id']\n",
    "    if 'reply_to_message_id' in message:\n",
    "        row['reply_to_message_id'] = message['reply_to_message_id']\n",
    "    else:\n",
    "        row['reply_to_message_id'] = pd.NA\n",
    "    message_text = ''\n",
    "    if 'text_entities' in message:\n",
    "        for text_entity in message['text_entities']:\n",
    "            message_text += ' ' + text_entity['text']\n",
    "    elif 'text' in message:\n",
    "        if type(message['text']) == str:\n",
    "            message_text = message['text']\n",
    "        else:\n",
    "            for text_entity in message['text']:\n",
    "                if type(text_entity) == str:\n",
    "                    message_text += ' ' + text_entity\n",
    "                else:\n",
    "                     message_text += ' ' + text_entity['text']\n",
    "    row['message_text'] = message_text\n",
    "    return row\n",
    "\n",
    "def clean(row):\n",
    "    line = row['message_text']\n",
    "    line = re.sub(r'http\\S+', '', line)\n",
    "    line = re.sub(r'\\s+', ' ', line)\n",
    "    line = line.strip()\n",
    "    \n",
    "    return line\n",
    "\n",
    "def find_replies(df_data, message_id) -> list:\n",
    "    result = []\n",
    "    reply_ids = df_data.loc[df_data.reply_to_message_id == message_id].message_id\n",
    "    for reply_id in reply_ids:\n",
    "        replies = find_replies(df_data, reply_id)\n",
    "        if len(replies) == 0:\n",
    "            replies.insert(0, reply_id)\n",
    "            replies.insert(0, message_id)\n",
    "            result.append(replies)\n",
    "        else:\n",
    "            for reply in replies:\n",
    "                reply.insert(0, message_id)\n",
    "                result.append(reply)\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_sequences_for_message(row, df_data):\n",
    "    message_id = row.message_id\n",
    "    reply_sequences = find_replies(df_data, message_id)\n",
    "    new_data = []\n",
    "    for seq in reply_sequences:\n",
    "        begin_from = 0\n",
    "        for i in range(1, len(seq)):\n",
    "            X_inidces = seq[begin_from:i]\n",
    "            y_indices = seq[i]\n",
    "            X = df_data[df_data.message_id.isin(X_inidces)].cleaned.str.cat(sep=config.SEQ_SEPARATOR)\n",
    "            y = df_data[df_data.message_id == y_indices].cleaned.iloc[0]\n",
    "            if len(X) > config.MAX_INPUT_TEXT_SEQ_LEN:\n",
    "                begin_from += 1\n",
    "            else:\n",
    "               new_data.append([X, y])\n",
    "    return new_data\n",
    "\n",
    "def prepare_dataset_from_tg_channel(file_name: str, parse_json: bool = False):\n",
    "    if parse_json:\n",
    "        with open('data/tg/'+tg_filename+'.json', 'r') as json_file:\n",
    "            print(f'Load json file {tg_filename}')\n",
    "            js = json.load(json_file)\n",
    "            print(f'Parse json file {tg_filename}')\n",
    "            messages_len = len(js['messages'])\n",
    "            df_data = pd.DataFrame(index=np.arange(messages_len), columns=['message_id'])\n",
    "            df_data = df_data.progress_apply(parse_message, messages=js['messages'], axis=1)\n",
    "            \n",
    "        df_data.to_csv(f'data/tg/{tg_filename}.csv', index=False)\n",
    "    else:\n",
    "        df_data = pd.read_csv(f'data/tg/{tg_filename}.csv')\n",
    "    print(f'Prepare dataset from file {tg_filename}')\n",
    "    df_data = df_data[:100_000]\n",
    "    df_data = df_data.dropna(subset=['message_text'])\n",
    "    df_data = df_data.drop_duplicates(subset=['message_text'], keep='first')\n",
    "    df_data = df_data[df_data.message_text.str.len() <= 256].copy()\n",
    "    # df_data = df_data.drop_duplicates(subset=['reply_to_message_id'], keep='first')\n",
    "    df_data['cleaned'] = df_data.progress_apply(clean, axis = 1)\n",
    "    df_data = df_data.loc[df_data.cleaned.str.len() >= 2]\n",
    "    # Find root messages and build sequences\n",
    "    sr_sequences = df_data.loc[~df_data.reply_to_message_id.isin(df_data.message_id)].progress_apply(build_sequences_for_message, df_data=df_data, axis=1)\n",
    "    sr_sequences = sr_sequences[sr_sequences.apply(lambda x: len(x) > 0)]\n",
    "    sr_sequences = sr_sequences.explode().apply(lambda x: pd.Series([x[0], x[1]]))\n",
    "    sr_sequences.columns = ['X', 'y']\n",
    "    sr_sequences.drop_duplicates(subset=['X'], keep='first', inplace=True)\n",
    "    sr_sequences.reset_index(drop=True, inplace=True)\n",
    "    print(f'Created {len(sr_sequences)} sequences for file {tg_filename}')\n",
    "    return sr_sequences\n",
    "    \n",
    "\n",
    "tg_filenames = ['vremya_valeri', 'israel_repatriation', 'better_data_community']\n",
    "df_dataset = pd.DataFrame(columns=['X', 'y'])\n",
    "for tg_filename in tg_filenames:\n",
    "    df_dataset = pd.concat([df_dataset, prepare_dataset_from_tg_channel(tg_filename, parse_json=False)], ignore_index=True)\n",
    "df_dataset.reset_index(drop=True, inplace=True)\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "BPE_MODEL_FILENAME = f'data/tg/{tg_filename}.yttm'\n",
    "TRAIN_TEXTS_FILENAME = f'data/tg/{tg_filename}.txt'\n",
    "\n",
    "EOS_ID = 3\n",
    "BOS_ID = 2\n",
    "\n",
    "separator = '<SEP>'\n",
    "separator = ''\n",
    "\n",
    "def write_line(row, file):\n",
    "    if row.cleaned:\n",
    "        file.write(row.cleaned+'\\n')\n",
    "    return row\n",
    "\n",
    "def decode_sample(input_ids) -> map:\n",
    "    return {\n",
    "        'encoder_text': self.decode_tensor(input_ids[0]['encoder_ids']),\n",
    "        'decoder_text': self.decode_tensor(input_ids[0]['decoder_ids']),\n",
    "        'target_text': self.decode_tensor(input_ids[1])\n",
    "    }\n",
    "    \n",
    "def decode_tensor(ids) -> str:\n",
    "    return self.tokenizer.decode(ids.cpu().detach().numpy().tolist())[0]      \n",
    "\n",
    "if True:\n",
    "    with open(TRAIN_TEXTS_FILENAME, 'w') as outf:\n",
    "        # df_data.apply(write_line, file = outf, axis=1)\n",
    "        df_questions.apply(write_line, file = outf, axis=1)\n",
    "        df_answers.apply(write_line, file = outf, axis=1)\n",
    "    yttm.BPE.train(data=TRAIN_TEXTS_FILENAME, vocab_size=8000, model=BPE_MODEL_FILENAME)\n",
    "tokenizer = yttm.BPE(BPE_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "class CharTokenizer():\n",
    "    def __init__(self, text_file_path) -> None:\n",
    "        with open(text_file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # here are all the unique characters that occur in this text\n",
    "        chars = sorted(list(set(text)))\n",
    "        self._vocab_size = len(chars)+1\n",
    "        # create a mapping from characters to integers\n",
    "        stoi = { ch:i+1 for i,ch in enumerate(chars) }\n",
    "        itos = { i+1:ch for i,ch in enumerate(chars) }\n",
    "        stoi['<PAD>'] = 0\n",
    "        itos[0] = '<PAD>'\n",
    "        self._encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "        self._decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "        \n",
    "    # def encode(self, sentences: List[str], bos=True, eos=True):\n",
    "    #     return [self._encode(sentence) for sentence in sentences]\n",
    "    \n",
    "    # def decode(self, sentences: List[str], bos=True, eos=True):\n",
    "    #     return [self._decode(sentence) for sentence in sentences]\n",
    "    \n",
    "    def encode(self, sentence: str, bos=True, eos=True):\n",
    "        return self._encode(sentence)\n",
    "    \n",
    "    def decode(self, sentence: str, bos=True, eos=True):\n",
    "        return self._decode(sentence)\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return self._vocab_size\n",
    "        \n",
    "tokenizer = CharTokenizer(TRAIN_TEXTS_FILENAME)\n",
    "tokenizer.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.mt5_model_name = \"cointegrated/rut5-small\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(config.mt5_model_name)\n",
    "\n",
    "def decode_sample(input_ids, skip_special_tokens=False) -> map:\n",
    "    return {\n",
    "        'encoder_text': decode_tensor(input_ids[0]['encoder_ids'], skip_special_tokens),\n",
    "        'decoder_text': decode_tensor(input_ids[0]['decoder_ids'], skip_special_tokens),\n",
    "        'target_text': decode_tensor(input_ids[1], skip_special_tokens)\n",
    "    }\n",
    "    \n",
    "def decode_tensor(ids, skip_special_tokens=False) -> str:\n",
    "    return tokenizer.decode(torch.where(ids != -100, ids, tokenizer.pad_token_id), skip_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "questions_tokenized = tokenizer.batch_encode_plus(df_dataset.X.to_list(), return_attention_mask=False).input_ids\n",
    "answers_tokenized = tokenizer.batch_encode_plus(df_dataset.y.to_list(), return_attention_mask=False).input_ids\n",
    "questions_lengs = [len(tokenized) for tokenized in questions_tokenized]\n",
    "answers_lengs = [len(tokenized) for tokenized in answers_tokenized]\n",
    "_, axes = plt.subplots(nrows=1, ncols=2, sharex=False, figsize=(15, 5))\n",
    "axes[0].hist(questions_lengs, bins=100);\n",
    "axes[0].set_xticks(np.arange(min(questions_lengs), max(questions_lengs)+1, 50.0));\n",
    "axes[1].hist(answers_lengs, bins=100);\n",
    "axes[1].set_xticks(np.arange(min(questions_lengs), max(questions_lengs)+1, 50.0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'encoder_ids': tensor([  259,  7472,  6334,  4013,   917,  5627, 10791, 17248,  3330,   259,\n",
      "         7236,  6131,   279,   261,   310,  5375,   833,   805,  7955,  3604,\n",
      "        11598,  5688,   433,   260, 11896,   995,  5907,   259,  9079,  8110,\n",
      "          833,  7423,  1965,  1066,   315,  5145, 12904,   261,   922,   259,\n",
      "          279,   315, 12314,  3725,  1625,   260,  1051,  5441,   315,  6390,\n",
      "         1323,  6136,  8371,   259,   279, 11029,  8371, 14425, 19765,   433,\n",
      "          267,  1617,   261,   966,   261,  4071,   259,   279,  5401, 16600,\n",
      "         6813,  2709,   399,   478,   559,   669, 16488,  1348,   388, 13373,\n",
      "          324,  3942,  2766,   262,   348,   371,   737,  2622,   419,   310,\n",
      "        12420,   291,   688,  5172, 14301,   892,   259,  1802,  5313,   966,\n",
      "          992,   259,  5627,   259,  8072,  5029,   587,   374, 10232,  2460,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'decoder_ids': tensor([   0, 2758, 7756,  892, 5464,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])}, tensor([2758, 7756,  892, 5464,    1, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "{'encoder_text': 'Когда становится совсем невмоготу читать статьи, на помощь приходят эспандеры. Крепкий хват вещь полезная как в борьбе, так и в качалке. На фото в ряд разминочные и тренировочные экспандеры: 60, 100, 120 и 140 фунтов<SEP>Будет сжатие Captain Crush 4 на запись? Слышал что его человек 100 во всем мире могут закрыть :)</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', 'decoder_text': '<pad> боюсь что нет<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', 'target_text': 'боюсь что нет</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'}\n"
     ]
    }
   ],
   "source": [
    "# %%script echo scipping\n",
    "\n",
    "config.ENCODER_SEQ_LEN = 200\n",
    "config.DECODER_SEQ_LEN = 100\n",
    "class LanguageModelDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, df_dataset: pd.DataFrame, is_hf=True) -> None:\n",
    "        super().__init__()\n",
    "        self.is_hf = is_hf\n",
    "        self.df_dataset = df_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.df_dataset.iloc[index]\n",
    "        \n",
    "        question_text = sample.X\n",
    "        answer_text = sample.y\n",
    "        \n",
    "        if self.is_hf:\n",
    "            question_tokens = self.tokenizer.encode(question_text, max_length= config.ENCODER_SEQ_LEN, truncation=True, padding='max_length',return_tensors='pt')\n",
    "            answer_tokens = self.tokenizer.encode('<pad>'+answer_text, max_length= config.DECODER_SEQ_LEN, truncation=True, padding='max_length',return_tensors='pt',add_special_tokens=False)\n",
    "            result_tokens = self.tokenizer.encode(answer_text, max_length= config.DECODER_SEQ_LEN, truncation=True, padding='max_length',return_tensors='pt')\n",
    "            src_ids = question_tokens.squeeze(0)\n",
    "            trg_ids = answer_tokens.squeeze(0)\n",
    "            res_ids = result_tokens.squeeze(0)\n",
    "            res_ids[res_ids == self.tokenizer.pad_token_id] = -100\n",
    "            return (\n",
    "                {'encoder_ids': src_ids, 'decoder_ids': trg_ids},\n",
    "                res_ids\n",
    "            )\n",
    "        else:\n",
    "            question_tokens = self.tokenizer.encode(question_text, bos=False, eos=False)\n",
    "            answer_tokens = self.tokenizer.encode(answer_text, bos=True, eos=True)\n",
    "            trg_tokens = answer_tokens[:-1]\n",
    "            res_tokens = answer_tokens[1:]\n",
    "            src_ids, trg_ids, res_ids = self.ensure_length(question_tokens, config.SEQ_LEN), self.ensure_length(trg_tokens, config.SEQ_LEN), self.ensure_length(res_tokens, config.SEQ_LEN)\n",
    "            src_ids = torch.tensor(src_ids)\n",
    "            trg_ids = torch.tensor(trg_ids)\n",
    "            res_ids = torch.tensor(res_ids)\n",
    "            return (\n",
    "                {'encoder_ids': src_ids, 'decoder_ids': trg_ids},\n",
    "                res_ids\n",
    "            )\n",
    "        \n",
    "train, val = train_test_split(df_dataset, test_size=0.2, random_state=SEED)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "train_dataset = LanguageModelDataset(tokenizer, train)\n",
    "val_dataset = LanguageModelDataset(tokenizer, val)\n",
    "sample = train_dataset[np.random.randint(0, train_dataset.__len__())]\n",
    "print(sample)\n",
    "print(decode_sample(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.64448 M parameters\n"
     ]
    }
   ],
   "source": [
    "# %%script echo skipping\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = T5ForConditionalGeneration.from_pretrained(config.mt5_model_name)\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        src_ids = src['encoder_ids']\n",
    "        trg_ids = src['decoder_ids']\n",
    "        \n",
    "        src_padding_mask = (src_ids != 0).type(torch.long)\n",
    "        trg_padding_mask = (trg_ids != 0).type(torch.long)\n",
    "        trg_padding_mask[:, 0] = 1\n",
    "        \n",
    "        output = self.backbone(input_ids=src_ids, attention_mask=src_padding_mask, decoder_input_ids=trg_ids, decoder_attention_mask=trg_padding_mask)\n",
    "\n",
    "        return output.logits\n",
    "    \n",
    "    def generate(self, seed_token_ids, max_len=40, return_hypotheses_n=5, beamsize=5):\n",
    "        output = []\n",
    "        result = self.backbone.generate(\n",
    "                    inputs=seed_token_ids['encoder_ids'].unsqueeze(0), \n",
    "                    num_return_sequences=return_hypotheses_n,  \n",
    "                    num_beams=beamsize, \n",
    "                    max_length=max_len,\n",
    "                    do_sample=True, \n",
    "                    return_dict_in_generate=True, \n",
    "                    output_scores=True, \n",
    "                )\n",
    "        for i in range(return_hypotheses_n):\n",
    "            output.append((result.sequences_scores[i].item(), result.sequences[i]))\n",
    "        return output\n",
    "\n",
    "model = TransformerModel().to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo scipping\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, embedding_dim: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, seq_len: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.nhead = nhead\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.token_embedding = nn.Embedding(ntoken, embedding_dim, padding_idx=0)\n",
    "        self.pos_encoder = nn.Embedding(seq_len, embedding_dim)\n",
    "        \n",
    "        \n",
    "        encoder_norm = nn.LayerNorm(embedding_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead, dim_feedforward=d_hid, dropout=dropout, batch_first=True, norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers, norm=encoder_norm)\n",
    "        \n",
    "        decoder_norm = nn.LayerNorm(embedding_dim)\n",
    "        decoder_layers = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=nhead, dim_feedforward=d_hid, dropout=dropout, batch_first=True, norm_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, nlayers, norm=decoder_norm)\n",
    "        \n",
    "        self.lm_head = nn.Linear(embedding_dim, ntoken)\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def src_mask(self, seq_len):\n",
    "        full_mask = torch.ones(seq_len, seq_len)\n",
    "        ignore_mask = torch.tril(full_mask) < 1\n",
    "        full_mask.masked_fill_(ignore_mask, float('-inf'))\n",
    "        full_mask.masked_fill_(~ignore_mask, 0)\n",
    "        return full_mask.to(device)\n",
    "    \n",
    "    def make_positional_encoding(self, max_length, embedding_size):\n",
    "        time = np.pi * torch.arange(0, max_length).float()\n",
    "        freq_dividers = torch.arange(1, embedding_size // 2 + 1).float()\n",
    "        inputs = time[:, None] / freq_dividers[None, :]\n",
    "        \n",
    "        result = torch.zeros(max_length, embedding_size)\n",
    "        result[:, 0::2] = torch.sin(inputs)\n",
    "        result[:, 1::2] = torch.cos(inputs)\n",
    "        return result.to(device)\n",
    "\n",
    "    def threeD_src_mask(self, n_heads, source_len, target_len, answers_start):\n",
    "        answers_start = answers_start.repeat_interleave(n_heads)\n",
    "        B = answers_start.shape[0]\n",
    "        full_mask = torch.ones(B, target_len, source_len)\n",
    "        ignore_mask = torch.stack([torch.tril(x, int(y)) for x,y in zip(full_mask, answers_start)]) < 1\n",
    "        full_mask.masked_fill_(ignore_mask, float('-inf'))\n",
    "        full_mask.masked_fill_(~ignore_mask, 0)\n",
    "        return full_mask\n",
    "    \n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        src_ids = src['encoder_ids']\n",
    "        trg_ids = src['decoder_ids']\n",
    "        srcB, srcT = src_ids.shape\n",
    "        trgB, trgT = trg_ids.shape\n",
    "        src_padding_mask = src_ids == 0\n",
    "        trg_padding_mask = trg_ids == 0\n",
    "        src_ids = self.token_embedding(src_ids) # * math.sqrt(self.embedding_dim)\n",
    "        trg_ids = self.token_embedding(trg_ids) # * math.sqrt(self.embedding_dim)\n",
    "        src_ids = src_ids + self.pos_encoder(torch.arange(srcT, device=device))\n",
    "        trg_ids = trg_ids + self.pos_encoder(torch.arange(trgT, device=device))\n",
    "        trg_attention_mask = self.src_mask(trgT)\n",
    "        encoder_mem = self.transformer_encoder(src_ids, src_key_padding_mask = src_padding_mask)\n",
    "        input_ids = self.transformer_decoder(tgt=trg_ids, memory = encoder_mem, tgt_mask = trg_attention_mask, tgt_key_padding_mask = trg_padding_mask, memory_key_padding_mask = src_padding_mask)\n",
    "        logits = self.lm_head(input_ids)\n",
    "\n",
    "        return logits\n",
    "\n",
    "seq_len = config.SEQ_LEN\n",
    "ntokens = tokenizer.vocab_size()  # size of vocabulary\n",
    "emsize = 480  # embedding dimension\n",
    "d_hid = 480  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 8  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "\n",
    "model = TransformerModel(ntoken = ntokens, embedding_dim=emsize, nhead=nhead, d_hid=d_hid, nlayers=nlayers, seq_len=seq_len, dropout=dropout)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generation={'epoch_i': -1, 'score': -1.0803672075271606, 'encoder_text': 'Может там какая-то нейросеть ответы шлет ))<SEP>Мне вот тоже интересно) а если вручную, то думаю анкету убрать. У меня там с ней 110 страниц получилось😹<SEP>110 это жестко) я джипегами приложил отдельно 9 документов, потом уже когда отправил понял что некоторые доки забыл, потом читаю люди пдфами шлют, вот и думаю че делать ждать или повторно слать<SEP>Ну вот и я думаю. Там наверное устанут их листать😂 их них 36 вроде анкета. Остальное доки', 'predicted_text': 'Да, я так понимаю, что если я не правильно поняла, то я не уверена что они не уверены, что они могут сделать анкету, а потом отправить и повторно слать и т.п. 🤷 ♀️🤷 ♀️🤷 ♀️🤷 ��', 'target_text': '74 дока да они в шоке там'}\n",
      "Sample generation={'epoch_i': -1, 'score': -1.2391160726547241, 'encoder_text': 'Может там какая-то нейросеть ответы шлет ))<SEP>Мне вот тоже интересно) а если вручную, то думаю анкету убрать. У меня там с ней 110 страниц получилось😹<SEP>110 это жестко) я джипегами приложил отдельно 9 документов, потом уже когда отправил понял что некоторые доки забыл, потом читаю люди пдфами шлют, вот и думаю че делать ждать или повторно слать<SEP>Ну вот и я думаю. Там наверное устанут их листать😂 их них 36 вроде анкета. Остальное доки', 'predicted_text': 'Спасибо большое за ответ!', 'target_text': '74 дока да они в шоке там'}\n",
      "Sample generation={'epoch_i': -1, 'score': -0.8571933507919312, 'encoder_text': 'подкажите, пожалуйста: 1. приходит ли смс с кодом для получения ТМ? мне пришел трекинг номер без смс. 2. если подавали на 2 паспорта, то доставка будет по одномк трекингу или двумя разными доставками?<SEP>Трекинг посылает почта, а смс МВД, каждый документ в отдельном конверте<SEP>Спасибо! А смс когда приходит?', 'predicted_text': 'Когда приходит смс, приходит смс для получения тз, а смс приходит по почте, а смс приходит по почте, а смс приходит по почте, а смс приходит по почте, а смс приходит по почте', 'target_text': 'Когда у письма изменится статус на \"выдано адресату\"'}\n",
      "Sample generation={'epoch_i': -1, 'score': -0.9255996346473694, 'encoder_text': 'подкажите, пожалуйста: 1. приходит ли смс с кодом для получения ТМ? мне пришел трекинг номер без смс. 2. если подавали на 2 паспорта, то доставка будет по одномк трекингу или двумя разными доставками?<SEP>Трекинг посылает почта, а смс МВД, каждый документ в отдельном конверте<SEP>Спасибо! А смс когда приходит?', 'predicted_text': 'Когда приходит смс на почту, а смс на почту приходит по почте, а смс приходит по почте', 'target_text': 'Когда у письма изменится статус на \"выдано адресату\"'}\n",
      "Sample generation={'epoch_i': -1, 'score': -0.9939466714859009, 'encoder_text': '(А вы нашли зачем нужно gonogo даёт тем, кто не претендует на бесплатные билеты? Жильё не дают. Страховку дают не факт что в связи с этим, и в любом случае она действует только до получения гражданства, и вообще фиг с ней. Что ещё?..)<SEP>Можно с консулом в аеропорту не вести беседы - я так поняла<SEP>Не получилось не вести беседы. Вчера в аэропорту консул был серьезный', 'predicted_text': 'Да, я так поняла, спасибо 🙏🏻', 'target_text': 'Марина, расскажите'}\n",
      "Sample generation={'epoch_i': -1, 'score': -1.0583263635635376, 'encoder_text': '(А вы нашли зачем нужно gonogo даёт тем, кто не претендует на бесплатные билеты? Жильё не дают. Страховку дают не факт что в связи с этим, и в любом случае она действует только до получения гражданства, и вообще фиг с ней. Что ещё?..)<SEP>Можно с консулом в аеропорту не вести беседы - я так поняла<SEP>Не получилось не вести беседы. Вчера в аэропорту консул был серьезный', 'predicted_text': 'Да, вот я и поняла, что если я не вести беседы, то я не могу вести беседы - я так понимаю, что если я не хочу вести беседы - я правильно понимаю, что я не хочу вести беседы - я так понимаю, что я не могу вести беседы.', 'target_text': 'Марина, расскажите'}\n"
     ]
    }
   ],
   "source": [
    "sample_indices = np.random.randint(0, len(val_dataset), 3, dtype=np.int32)\n",
    "samples = []\n",
    "for sample_idx in sample_indices:\n",
    "    sample = val_dataset[sample_idx]\n",
    "    sample_text = decode_sample(sample, skip_special_tokens=True)\n",
    "    samples.append(\n",
    "        {\n",
    "            'encoder_ids': sample[0]['encoder_ids'],\n",
    "            'decoder_ids': sample[0]['decoder_ids'],\n",
    "            'target_ids': sample[1],\n",
    "            'encoder_text': sample_text['encoder_text'],\n",
    "            'target_text': sample_text['target_text'],\n",
    "        }\n",
    "    )\n",
    "\n",
    "def generate_sample(epoch_i, model, tensorboard: SummaryWriterHelper=None):\n",
    "    for sample in samples:\n",
    "        encoder_ids = sample['encoder_ids'].to(device)\n",
    "        decoder_ids = sample['decoder_ids'][:5].to(device)\n",
    "        seed_token_ids = { 'encoder_ids' : encoder_ids, 'decoder_ids' : decoder_ids }\n",
    "        hypothesis_list = model.generate(seed_token_ids=seed_token_ids, max_len=config.DECODER_SEQ_LEN, return_hypotheses_n=2, beamsize=5)\n",
    "        for score, hypothesis in hypothesis_list:\n",
    "            predicted_text = decode_tensor(hypothesis, skip_special_tokens=True)\n",
    "            sample_table = {\n",
    "                'epoch_i': epoch_i,\n",
    "                'score': score,\n",
    "                'encoder_text' : sample['encoder_text'],\n",
    "                'predicted_text' : predicted_text,\n",
    "                'target_text' : sample['target_text'],\n",
    "            }\n",
    "            print(f\"Sample generation={sample_table}\")\n",
    "            if tensorboard:\n",
    "                tensorboard.add_text(\"sample\", str(sample_table))\n",
    "generate_sample(-1, model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.BATCH_SIZE = 6\n",
    "\n",
    "def cross_entropy_loss(predict, target):\n",
    "    B, T, C = predict.shape\n",
    "    predict = predict.view(B*T, C)\n",
    "    target = target.view(B*T)\n",
    "    loss = F.cross_entropy(predict, target, ignore_index=-100)\n",
    "    return loss\n",
    "\n",
    "writer = SummaryWriterHelper()\n",
    "\n",
    "def scheduler(optim): return \\\n",
    "    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "best_val_loss, best_model = train_eval_loop(model=model,\n",
    "                                            device=device,\n",
    "                                            train_dataset=train_dataset,\n",
    "                                            val_dataset=val_dataset,\n",
    "                                            criterion=cross_entropy_loss,\n",
    "                                            lr=1e-4,\n",
    "                                            epoch_n=200,\n",
    "                                            batch_size=config.BATCH_SIZE,\n",
    "                                            l2_reg_alpha=1e-2,\n",
    "                                            lr_scheduler_ctor=scheduler,\n",
    "                                            shuffle_train=True,\n",
    "                                            tensorboard=writer,\n",
    "                                            on_epoch_cb=generate_sample\n",
    "                                            )\n",
    "print(\"saving best model...\")\n",
    "torch.save(best_model.state_dict(), 'data/model.pth')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'data/model.pth', map_location=device))\n",
    "best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> Да, вот и я думал, что это новая версия, а я и не уверена, что может быть ещё и не сильно предсказуемая.</s>'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('Странная модель получилась', max_length= config.ENCODER_SEQ_LEN, truncation=True, padding='max_length',return_tensors='pt').to(device)\n",
    "output_ids = model.backbone.generate(\n",
    "                    inputs=input_ids, \n",
    "                    num_return_sequences=1,  \n",
    "                    num_beams=5, \n",
    "                    max_length=config.DECODER_SEQ_LEN,\n",
    "                    do_sample=True,\n",
    "                )\n",
    "tokenizer.decode(output_ids[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sharedenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
