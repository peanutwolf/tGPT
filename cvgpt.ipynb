{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 21:42:08.647264: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-05 21:42:09.462745: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-05 21:42:09.462836: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-05 21:42:09.462845: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import youtokentome as yttm\n",
    "from dotmap import DotMap\n",
    "\n",
    "from dnnutils.generate import BeamGenerator, ensure_length\n",
    "from dnnutils.nn_helper import init_random_seed, train_eval_loop, predict_with_model\n",
    "from dnnutils.tensorboard import SummaryWriterHelper\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [6, 4]\n",
    "tqdm.pandas()\n",
    "\n",
    "SEED = 42\n",
    "init_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evigurskiy/.venvs/sharedenv/lib/python3.10/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DotMap()\n",
    "config.MAX_INPUT_TEXT_SEQ_LEN = 512\n",
    "config.SEQ_SEPARATOR = '<SEP>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare dataset from file vremya_valeri\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16218/16218 [00:00<00:00, 55358.03it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5747/5747 [00:15<00:00, 370.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6099 sequences for file vremya_valeri\n",
      "Prepare dataset from file israel_repatriation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80482/80482 [00:01<00:00, 79079.26it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31181/31181 [02:02<00:00, 253.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 34174 sequences for file israel_repatriation\n",
      "Prepare dataset from file better_data_community\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6697/6697 [00:00<00:00, 49456.31it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3688/3688 [00:05<00:00, 665.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2286 sequences for file better_data_community\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–ü–æ–ø—Ä–æ–±—É—é –¥–æ–±–∞–≤–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å</td>\n",
       "      <td>–ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ü–æ–ø—Ä–æ–±—É—é –¥–æ–±–∞–≤–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å&lt;S...</td>\n",
       "      <td>–ü—Ä–µ–∫—Ä–∞—Å–Ω–æ, –∂–¥—ë–º</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data-driven team management</td>\n",
       "      <td>–≠—Ç–æ —Å–∫–æ—Ä–µ–µ –¥–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–π. –ê —Ç—É—Ç –≤–æ–ø—Ä–æ—Å –≤ –ø—Ä–∞–≤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–ù–µ–¥–∞–≤–Ω–æ —è —É–ª–µ—Ç–∞–ª –∏–∑ –õ–æ–Ω–¥–æ–Ω–∞ –∏ –ø—Ä–∏–ª–µ—Ç–∞–ª –≤ –ú–æ—Å–∫–≤...</td>\n",
       "      <td>–ö–∞–∂–µ—Ç—Å—è —á—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –±—ã–ª–æ –±—ã —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –¥–≤–∞ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–ù–µ–¥–∞–≤–Ω–æ —è —É–ª–µ—Ç–∞–ª –∏–∑ –õ–æ–Ω–¥–æ–Ω–∞ –∏ –ø—Ä–∏–ª–µ—Ç–∞–ª –≤ –ú–æ—Å–∫–≤...</td>\n",
       "      <td>–í —Ç–∞–∫–æ–º —Å–ª—É—á–∞–µ –∏ –∫–ª–∞—Å—Å—ã –¥–æ–º–æ–≤ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–æ–ø–æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42554</th>\n",
       "      <td>–ï—â—ë –µ—Å—Ç—å —Å—Ç—Ä–∞—à–Ω—ã–µ —Å–ª–æ–≤–∞ metric learning –∏ trip...</td>\n",
       "      <td>Contrastive learning –ø–æ—Å–º–æ—Ç—Ä–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42555</th>\n",
       "      <td>–ù–µ –Ω–∞—à–µ–ª –∏–Ω—Ñ—É –æ–± Eqvilent. –ß–µ–º –∑–∞–Ω–∏–º–∞–µ—Ç–µ—Å—å, –∫–∞...</td>\n",
       "      <td>–≤–æ—Ç –∂–µ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42556</th>\n",
       "      <td>—É –Ω–∞—Å –µ—Å—Ç—å –¥–∂—É–Ω –≤–∞–∫–∞–Ω—Å–∏–∏, –º–æ–∂–µ—à—å –ø–æ–¥–∞—Ç—å—Å—è –∏ –ø–∏...</td>\n",
       "      <td>–° –≤–∞–º–∏ —è —É–∂–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42557</th>\n",
       "      <td>–ù–∞ –¥–∑–µ–Ω–µ)))</td>\n",
       "      <td>–ê –≤–∏–¥–µ–æ –Ω–∞ —Ä—É—Ç—É–±–µ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42558</th>\n",
       "      <td>–ü–æ–∫–∞ –±—ã–ª 1—ã–π —Å–æ–±–µ—Å</td>\n",
       "      <td>–Ω–∞–π—Å</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42559 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       X  \\\n",
       "0           –ü–æ–ø—Ä–æ–±—É—é –¥–æ–±–∞–≤–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å   \n",
       "1      –ü–æ–ø—Ä–æ–±—É—é –¥–æ–±–∞–≤–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å<S...   \n",
       "2                            data-driven team management   \n",
       "3      –ù–µ–¥–∞–≤–Ω–æ —è —É–ª–µ—Ç–∞–ª –∏–∑ –õ–æ–Ω–¥–æ–Ω–∞ –∏ –ø—Ä–∏–ª–µ—Ç–∞–ª –≤ –ú–æ—Å–∫–≤...   \n",
       "4      –ù–µ–¥–∞–≤–Ω–æ —è —É–ª–µ—Ç–∞–ª –∏–∑ –õ–æ–Ω–¥–æ–Ω–∞ –∏ –ø—Ä–∏–ª–µ—Ç–∞–ª –≤ –ú–æ—Å–∫–≤...   \n",
       "...                                                  ...   \n",
       "42554  –ï—â—ë –µ—Å—Ç—å —Å—Ç—Ä–∞—à–Ω—ã–µ —Å–ª–æ–≤–∞ metric learning –∏ trip...   \n",
       "42555  –ù–µ –Ω–∞—à–µ–ª –∏–Ω—Ñ—É –æ–± Eqvilent. –ß–µ–º –∑–∞–Ω–∏–º–∞–µ—Ç–µ—Å—å, –∫–∞...   \n",
       "42556  —É –Ω–∞—Å –µ—Å—Ç—å –¥–∂—É–Ω –≤–∞–∫–∞–Ω—Å–∏–∏, –º–æ–∂–µ—à—å –ø–æ–¥–∞—Ç—å—Å—è –∏ –ø–∏...   \n",
       "42557                                        –ù–∞ –¥–∑–µ–Ω–µ)))   \n",
       "42558                                 –ü–æ–∫–∞ –±—ã–ª 1—ã–π —Å–æ–±–µ—Å   \n",
       "\n",
       "                                                       y  \n",
       "0                                         –ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç...  \n",
       "1                                        –ü—Ä–µ–∫—Ä–∞—Å–Ω–æ, –∂–¥—ë–º  \n",
       "2      –≠—Ç–æ —Å–∫–æ—Ä–µ–µ –¥–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–π. –ê —Ç—É—Ç –≤–æ–ø—Ä–æ—Å –≤ –ø—Ä–∞–≤...  \n",
       "3      –ö–∞–∂–µ—Ç—Å—è —á—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –±—ã–ª–æ –±—ã —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –¥–≤–∞ ...  \n",
       "4      –í —Ç–∞–∫–æ–º —Å–ª—É—á–∞–µ –∏ –∫–ª–∞—Å—Å—ã –¥–æ–º–æ–≤ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–æ–ø–æ...  \n",
       "...                                                  ...  \n",
       "42554                      Contrastive learning –ø–æ—Å–º–æ—Ç—Ä–∏  \n",
       "42555                                             –≤–æ—Ç –∂–µ  \n",
       "42556                           –° –≤–∞–º–∏ —è —É–∂–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ)  \n",
       "42557                                  –ê –≤–∏–¥–µ–æ –Ω–∞ —Ä—É—Ç—É–±–µ  \n",
       "42558                                               –Ω–∞–π—Å  \n",
       "\n",
       "[42559 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dataset sequences\n",
    "\n",
    "df_dataset = pd.DataFrame(columns=['X', 'y'])\n",
    "\n",
    "def parse_message(row, messages):\n",
    "    row_idx = row.name\n",
    "    message = messages[row_idx]\n",
    "    row['message_id'] = message['id']\n",
    "    row['date'] = pd.to_datetime(message['date'])\n",
    "    if 'from' not in message:\n",
    "        return row\n",
    "    row['from'] = message['from']\n",
    "    row['from_id'] = message['from_id']\n",
    "    if 'reply_to_message_id' in message:\n",
    "        row['reply_to_message_id'] = message['reply_to_message_id']\n",
    "    else:\n",
    "        row['reply_to_message_id'] = pd.NA\n",
    "    message_text = ''\n",
    "    if 'text_entities' in message:\n",
    "        for text_entity in message['text_entities']:\n",
    "            message_text += ' ' + text_entity['text']\n",
    "    elif 'text' in message:\n",
    "        if type(message['text']) == str:\n",
    "            message_text = message['text']\n",
    "        else:\n",
    "            for text_entity in message['text']:\n",
    "                if type(text_entity) == str:\n",
    "                    message_text += ' ' + text_entity\n",
    "                else:\n",
    "                     message_text += ' ' + text_entity['text']\n",
    "    row['message_text'] = message_text\n",
    "    return row\n",
    "\n",
    "def clean(row):\n",
    "    line = row['message_text']\n",
    "    line = re.sub(r'http\\S+', '', line)\n",
    "    line = re.sub(r'\\s+', ' ', line)\n",
    "    line = line.strip()\n",
    "    \n",
    "    return line\n",
    "\n",
    "def find_replies(df_data, message_id) -> list:\n",
    "    result = []\n",
    "    reply_ids = df_data.loc[df_data.reply_to_message_id == message_id].message_id\n",
    "    for reply_id in reply_ids:\n",
    "        replies = find_replies(df_data, reply_id)\n",
    "        if len(replies) == 0:\n",
    "            replies.insert(0, reply_id)\n",
    "            replies.insert(0, message_id)\n",
    "            result.append(replies)\n",
    "        else:\n",
    "            for reply in replies:\n",
    "                reply.insert(0, message_id)\n",
    "                result.append(reply)\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_sequences_for_message(row, df_data):\n",
    "    message_id = row.message_id\n",
    "    reply_sequences = find_replies(df_data, message_id)\n",
    "    new_data = []\n",
    "    for seq in reply_sequences:\n",
    "        begin_from = 0\n",
    "        for i in range(1, len(seq)):\n",
    "            X_inidces = seq[begin_from:i]\n",
    "            y_indices = seq[i]\n",
    "            X = df_data[df_data.message_id.isin(X_inidces)].cleaned.str.cat(sep=config.SEQ_SEPARATOR)\n",
    "            y = df_data[df_data.message_id == y_indices].cleaned.iloc[0]\n",
    "            if len(X) > config.MAX_INPUT_TEXT_SEQ_LEN:\n",
    "                begin_from += 1\n",
    "            else:\n",
    "               new_data.append([X, y])\n",
    "    return new_data\n",
    "\n",
    "def prepare_dataset_from_tg_channel(file_name: str, parse_json: bool = False):\n",
    "    if parse_json:\n",
    "        with open('data/tg/'+tg_filename+'.json', 'r') as json_file:\n",
    "            print(f'Load json file {tg_filename}')\n",
    "            js = json.load(json_file)\n",
    "            print(f'Parse json file {tg_filename}')\n",
    "            messages_len = len(js['messages'])\n",
    "            df_data = pd.DataFrame(index=np.arange(messages_len), columns=['message_id'])\n",
    "            df_data = df_data.progress_apply(parse_message, messages=js['messages'], axis=1)\n",
    "            \n",
    "        df_data.to_csv(f'data/tg/{tg_filename}.csv', index=False)\n",
    "    else:\n",
    "        df_data = pd.read_csv(f'data/tg/{tg_filename}.csv')\n",
    "    print(f'Prepare dataset from file {tg_filename}')\n",
    "    df_data = df_data[:100_000]\n",
    "    df_data = df_data.dropna(subset=['message_text'])\n",
    "    df_data = df_data.drop_duplicates(subset=['message_text'], keep='first')\n",
    "    df_data = df_data[df_data.message_text.str.len() <= 256].copy()\n",
    "    # df_data = df_data.drop_duplicates(subset=['reply_to_message_id'], keep='first')\n",
    "    df_data['cleaned'] = df_data.progress_apply(clean, axis = 1)\n",
    "    df_data = df_data.loc[df_data.cleaned.str.len() >= 2]\n",
    "    # Find root messages and build sequences\n",
    "    sr_sequences = df_data.loc[~df_data.reply_to_message_id.isin(df_data.message_id)].progress_apply(build_sequences_for_message, df_data=df_data, axis=1)\n",
    "    sr_sequences = sr_sequences[sr_sequences.apply(lambda x: len(x) > 0)]\n",
    "    sr_sequences = sr_sequences.explode().apply(lambda x: pd.Series([x[0], x[1]]))\n",
    "    sr_sequences.columns = ['X', 'y']\n",
    "    sr_sequences.drop_duplicates(subset=['X'], keep='first', inplace=True)\n",
    "    sr_sequences.reset_index(drop=True, inplace=True)\n",
    "    print(f'Created {len(sr_sequences)} sequences for file {tg_filename}')\n",
    "    return sr_sequences\n",
    "    \n",
    "\n",
    "tg_filenames = ['vremya_valeri', 'israel_repatriation', 'better_data_community']\n",
    "df_dataset = pd.DataFrame(columns=['X', 'y'])\n",
    "for tg_filename in tg_filenames:\n",
    "    df_dataset = pd.concat([df_dataset, prepare_dataset_from_tg_channel(tg_filename, parse_json=False)], ignore_index=True)\n",
    "df_dataset.reset_index(drop=True, inplace=True)\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "BPE_MODEL_FILENAME = f'data/tg/{tg_filename}.yttm'\n",
    "TRAIN_TEXTS_FILENAME = f'data/tg/{tg_filename}.txt'\n",
    "\n",
    "EOS_ID = 3\n",
    "BOS_ID = 2\n",
    "\n",
    "separator = '<SEP>'\n",
    "separator = ''\n",
    "\n",
    "def write_line(row, file):\n",
    "    if row.cleaned:\n",
    "        file.write(row.cleaned+'\\n')\n",
    "    return row\n",
    "\n",
    "def decode_sample(input_ids) -> map:\n",
    "    return {\n",
    "        'encoder_text': self.decode_tensor(input_ids[0]['encoder_ids']),\n",
    "        'decoder_text': self.decode_tensor(input_ids[0]['decoder_ids']),\n",
    "        'target_text': self.decode_tensor(input_ids[1])\n",
    "    }\n",
    "    \n",
    "def decode_tensor(ids) -> str:\n",
    "    return self.tokenizer.decode(ids.cpu().detach().numpy().tolist())[0]      \n",
    "\n",
    "if True:\n",
    "    with open(TRAIN_TEXTS_FILENAME, 'w') as outf:\n",
    "        # df_data.apply(write_line, file = outf, axis=1)\n",
    "        df_questions.apply(write_line, file = outf, axis=1)\n",
    "        df_answers.apply(write_line, file = outf, axis=1)\n",
    "    yttm.BPE.train(data=TRAIN_TEXTS_FILENAME, vocab_size=8000, model=BPE_MODEL_FILENAME)\n",
    "tokenizer = yttm.BPE(BPE_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "class CharTokenizer():\n",
    "    def __init__(self, text_file_path) -> None:\n",
    "        with open(text_file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # here are all the unique characters that occur in this text\n",
    "        chars = sorted(list(set(text)))\n",
    "        self._vocab_size = len(chars)+1\n",
    "        # create a mapping from characters to integers\n",
    "        stoi = { ch:i+1 for i,ch in enumerate(chars) }\n",
    "        itos = { i+1:ch for i,ch in enumerate(chars) }\n",
    "        stoi['<PAD>'] = 0\n",
    "        itos[0] = '<PAD>'\n",
    "        self._encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "        self._decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "        \n",
    "    # def encode(self, sentences: List[str], bos=True, eos=True):\n",
    "    #     return [self._encode(sentence) for sentence in sentences]\n",
    "    \n",
    "    # def decode(self, sentences: List[str], bos=True, eos=True):\n",
    "    #     return [self._decode(sentence) for sentence in sentences]\n",
    "    \n",
    "    def encode(self, sentence: str, bos=True, eos=True):\n",
    "        return self._encode(sentence)\n",
    "    \n",
    "    def decode(self, sentence: str, bos=True, eos=True):\n",
    "        return self._decode(sentence)\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return self._vocab_size\n",
    "        \n",
    "tokenizer = CharTokenizer(TRAIN_TEXTS_FILENAME)\n",
    "tokenizer.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.mt5_model_name = \"cointegrated/rut5-small\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(config.mt5_model_name)\n",
    "\n",
    "def decode_sample(input_ids, skip_special_tokens=False) -> map:\n",
    "    return {\n",
    "        'encoder_text': decode_tensor(input_ids[0]['encoder_ids'], skip_special_tokens),\n",
    "        'decoder_text': decode_tensor(input_ids[0]['decoder_ids'], skip_special_tokens),\n",
    "        'target_text': decode_tensor(input_ids[1], skip_special_tokens)\n",
    "    }\n",
    "    \n",
    "def decode_tensor(ids, skip_special_tokens=False) -> str:\n",
    "    return tokenizer.decode(torch.where(ids != -100, ids, tokenizer.pad_token_id), skip_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "questions_tokenized = tokenizer.batch_encode_plus(df_dataset.X.to_list(), return_attention_mask=False).input_ids\n",
    "answers_tokenized = tokenizer.batch_encode_plus(df_dataset.y.to_list(), return_attention_mask=False).input_ids\n",
    "questions_lengs = [len(tokenized) for tokenized in questions_tokenized]\n",
    "answers_lengs = [len(tokenized) for tokenized in answers_tokenized]\n",
    "_, axes = plt.subplots(nrows=1, ncols=2, sharex=False, figsize=(15, 5))\n",
    "axes[0].hist(questions_lengs, bins=100);\n",
    "axes[0].set_xticks(np.arange(min(questions_lengs), max(questions_lengs)+1, 50.0));\n",
    "axes[1].hist(answers_lengs, bins=100);\n",
    "axes[1].set_xticks(np.arange(min(questions_lengs), max(questions_lengs)+1, 50.0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'encoder_ids': tensor([  259,  7472,  6334,  4013,   917,  5627, 10791, 17248,  3330,   259,\n",
      "         7236,  6131,   279,   261,   310,  5375,   833,   805,  7955,  3604,\n",
      "        11598,  5688,   433,   260, 11896,   995,  5907,   259,  9079,  8110,\n",
      "          833,  7423,  1965,  1066,   315,  5145, 12904,   261,   922,   259,\n",
      "          279,   315, 12314,  3725,  1625,   260,  1051,  5441,   315,  6390,\n",
      "         1323,  6136,  8371,   259,   279, 11029,  8371, 14425, 19765,   433,\n",
      "          267,  1617,   261,   966,   261,  4071,   259,   279,  5401, 16600,\n",
      "         6813,  2709,   399,   478,   559,   669, 16488,  1348,   388, 13373,\n",
      "          324,  3942,  2766,   262,   348,   371,   737,  2622,   419,   310,\n",
      "        12420,   291,   688,  5172, 14301,   892,   259,  1802,  5313,   966,\n",
      "          992,   259,  5627,   259,  8072,  5029,   587,   374, 10232,  2460,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'decoder_ids': tensor([   0, 2758, 7756,  892, 5464,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])}, tensor([2758, 7756,  892, 5464,    1, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "{'encoder_text': '–ö–æ–≥–¥–∞ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Å–æ–≤—Å–µ–º –Ω–µ–≤–º–æ–≥–æ—Ç—É —á–∏—Ç–∞—Ç—å —Å—Ç–∞—Ç—å–∏, –Ω–∞ –ø–æ–º–æ—â—å –ø—Ä–∏—Ö–æ–¥—è—Ç —ç—Å–ø–∞–Ω–¥–µ—Ä—ã. –ö—Ä–µ–ø–∫–∏–π —Ö–≤–∞—Ç –≤–µ—â—å –ø–æ–ª–µ–∑–Ω–∞—è –∫–∞–∫ –≤ –±–æ—Ä—å–±–µ, —Ç–∞–∫ –∏ –≤ –∫–∞—á–∞–ª–∫–µ. –ù–∞ —Ñ–æ—Ç–æ –≤ —Ä—è–¥ —Ä–∞–∑–º–∏–Ω–æ—á–Ω—ã–µ –∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ —ç–∫—Å–ø–∞–Ω–¥–µ—Ä—ã: 60, 100, 120 –∏ 140 —Ñ—É–Ω—Ç–æ–≤<SEP>–ë—É–¥–µ—Ç —Å–∂–∞—Ç–∏–µ Captain Crush 4 –Ω–∞ –∑–∞–ø–∏—Å—å? –°–ª—ã—à–∞–ª —á—Ç–æ –µ–≥–æ —á–µ–ª–æ–≤–µ–∫ 100 –≤–æ –≤—Å–µ–º –º–∏—Ä–µ –º–æ–≥—É—Ç –∑–∞–∫—Ä—ã—Ç—å :)</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', 'decoder_text': '<pad> –±–æ—é—Å—å —á—Ç–æ –Ω–µ—Ç<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', 'target_text': '–±–æ—é—Å—å —á—Ç–æ –Ω–µ—Ç</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'}\n"
     ]
    }
   ],
   "source": [
    "# %%script echo scipping\n",
    "\n",
    "config.ENCODER_SEQ_LEN = 200\n",
    "config.DECODER_SEQ_LEN = 100\n",
    "class LanguageModelDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, df_dataset: pd.DataFrame, is_hf=True) -> None:\n",
    "        super().__init__()\n",
    "        self.is_hf = is_hf\n",
    "        self.df_dataset = df_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.df_dataset.iloc[index]\n",
    "        \n",
    "        question_text = sample.X\n",
    "        answer_text = sample.y\n",
    "        \n",
    "        if self.is_hf:\n",
    "            question_tokens = self.tokenizer.encode(question_text, max_length= config.ENCODER_SEQ_LEN, truncation=True, padding='max_length',return_tensors='pt')\n",
    "            answer_tokens = self.tokenizer.encode('<pad>'+answer_text, max_length= config.DECODER_SEQ_LEN, truncation=True, padding='max_length',return_tensors='pt',add_special_tokens=False)\n",
    "            result_tokens = self.tokenizer.encode(answer_text, max_length= config.DECODER_SEQ_LEN, truncation=True, padding='max_length',return_tensors='pt')\n",
    "            src_ids = question_tokens.squeeze(0)\n",
    "            trg_ids = answer_tokens.squeeze(0)\n",
    "            res_ids = result_tokens.squeeze(0)\n",
    "            res_ids[res_ids == self.tokenizer.pad_token_id] = -100\n",
    "            return (\n",
    "                {'encoder_ids': src_ids, 'decoder_ids': trg_ids},\n",
    "                res_ids\n",
    "            )\n",
    "        else:\n",
    "            question_tokens = self.tokenizer.encode(question_text, bos=False, eos=False)\n",
    "            answer_tokens = self.tokenizer.encode(answer_text, bos=True, eos=True)\n",
    "            trg_tokens = answer_tokens[:-1]\n",
    "            res_tokens = answer_tokens[1:]\n",
    "            src_ids, trg_ids, res_ids = self.ensure_length(question_tokens, config.SEQ_LEN), self.ensure_length(trg_tokens, config.SEQ_LEN), self.ensure_length(res_tokens, config.SEQ_LEN)\n",
    "            src_ids = torch.tensor(src_ids)\n",
    "            trg_ids = torch.tensor(trg_ids)\n",
    "            res_ids = torch.tensor(res_ids)\n",
    "            return (\n",
    "                {'encoder_ids': src_ids, 'decoder_ids': trg_ids},\n",
    "                res_ids\n",
    "            )\n",
    "        \n",
    "train, val = train_test_split(df_dataset, test_size=0.2, random_state=SEED)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "train_dataset = LanguageModelDataset(tokenizer, train)\n",
    "val_dataset = LanguageModelDataset(tokenizer, val)\n",
    "sample = train_dataset[np.random.randint(0, train_dataset.__len__())]\n",
    "print(sample)\n",
    "print(decode_sample(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.64448 M parameters\n"
     ]
    }
   ],
   "source": [
    "# %%script echo skipping\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = T5ForConditionalGeneration.from_pretrained(config.mt5_model_name)\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        src_ids = src['encoder_ids']\n",
    "        trg_ids = src['decoder_ids']\n",
    "        \n",
    "        src_padding_mask = (src_ids != 0).type(torch.long)\n",
    "        trg_padding_mask = (trg_ids != 0).type(torch.long)\n",
    "        trg_padding_mask[:, 0] = 1\n",
    "        \n",
    "        output = self.backbone(input_ids=src_ids, attention_mask=src_padding_mask, decoder_input_ids=trg_ids, decoder_attention_mask=trg_padding_mask)\n",
    "\n",
    "        return output.logits\n",
    "    \n",
    "    def generate(self, seed_token_ids, max_len=40, return_hypotheses_n=5, beamsize=5):\n",
    "        output = []\n",
    "        result = self.backbone.generate(\n",
    "                    inputs=seed_token_ids['encoder_ids'].unsqueeze(0), \n",
    "                    num_return_sequences=return_hypotheses_n,  \n",
    "                    num_beams=beamsize, \n",
    "                    max_length=max_len,\n",
    "                    do_sample=True, \n",
    "                    return_dict_in_generate=True, \n",
    "                    output_scores=True, \n",
    "                )\n",
    "        for i in range(return_hypotheses_n):\n",
    "            output.append((result.sequences_scores[i].item(), result.sequences[i]))\n",
    "        return output\n",
    "\n",
    "model = TransformerModel().to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo scipping\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, embedding_dim: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, seq_len: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.nhead = nhead\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.token_embedding = nn.Embedding(ntoken, embedding_dim, padding_idx=0)\n",
    "        self.pos_encoder = nn.Embedding(seq_len, embedding_dim)\n",
    "        \n",
    "        \n",
    "        encoder_norm = nn.LayerNorm(embedding_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead, dim_feedforward=d_hid, dropout=dropout, batch_first=True, norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers, norm=encoder_norm)\n",
    "        \n",
    "        decoder_norm = nn.LayerNorm(embedding_dim)\n",
    "        decoder_layers = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=nhead, dim_feedforward=d_hid, dropout=dropout, batch_first=True, norm_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, nlayers, norm=decoder_norm)\n",
    "        \n",
    "        self.lm_head = nn.Linear(embedding_dim, ntoken)\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def src_mask(self, seq_len):\n",
    "        full_mask = torch.ones(seq_len, seq_len)\n",
    "        ignore_mask = torch.tril(full_mask) < 1\n",
    "        full_mask.masked_fill_(ignore_mask, float('-inf'))\n",
    "        full_mask.masked_fill_(~ignore_mask, 0)\n",
    "        return full_mask.to(device)\n",
    "    \n",
    "    def make_positional_encoding(self, max_length, embedding_size):\n",
    "        time = np.pi * torch.arange(0, max_length).float()\n",
    "        freq_dividers = torch.arange(1, embedding_size // 2 + 1).float()\n",
    "        inputs = time[:, None] / freq_dividers[None, :]\n",
    "        \n",
    "        result = torch.zeros(max_length, embedding_size)\n",
    "        result[:, 0::2] = torch.sin(inputs)\n",
    "        result[:, 1::2] = torch.cos(inputs)\n",
    "        return result.to(device)\n",
    "\n",
    "    def threeD_src_mask(self, n_heads, source_len, target_len, answers_start):\n",
    "        answers_start = answers_start.repeat_interleave(n_heads)\n",
    "        B = answers_start.shape[0]\n",
    "        full_mask = torch.ones(B, target_len, source_len)\n",
    "        ignore_mask = torch.stack([torch.tril(x, int(y)) for x,y in zip(full_mask, answers_start)]) < 1\n",
    "        full_mask.masked_fill_(ignore_mask, float('-inf'))\n",
    "        full_mask.masked_fill_(~ignore_mask, 0)\n",
    "        return full_mask\n",
    "    \n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        src_ids = src['encoder_ids']\n",
    "        trg_ids = src['decoder_ids']\n",
    "        srcB, srcT = src_ids.shape\n",
    "        trgB, trgT = trg_ids.shape\n",
    "        src_padding_mask = src_ids == 0\n",
    "        trg_padding_mask = trg_ids == 0\n",
    "        src_ids = self.token_embedding(src_ids) # * math.sqrt(self.embedding_dim)\n",
    "        trg_ids = self.token_embedding(trg_ids) # * math.sqrt(self.embedding_dim)\n",
    "        src_ids = src_ids + self.pos_encoder(torch.arange(srcT, device=device))\n",
    "        trg_ids = trg_ids + self.pos_encoder(torch.arange(trgT, device=device))\n",
    "        trg_attention_mask = self.src_mask(trgT)\n",
    "        encoder_mem = self.transformer_encoder(src_ids, src_key_padding_mask = src_padding_mask)\n",
    "        input_ids = self.transformer_decoder(tgt=trg_ids, memory = encoder_mem, tgt_mask = trg_attention_mask, tgt_key_padding_mask = trg_padding_mask, memory_key_padding_mask = src_padding_mask)\n",
    "        logits = self.lm_head(input_ids)\n",
    "\n",
    "        return logits\n",
    "\n",
    "seq_len = config.SEQ_LEN\n",
    "ntokens = tokenizer.vocab_size()  # size of vocabulary\n",
    "emsize = 480  # embedding dimension\n",
    "d_hid = 480  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 8  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "\n",
    "model = TransformerModel(ntoken = ntokens, embedding_dim=emsize, nhead=nhead, d_hid=d_hid, nlayers=nlayers, seq_len=seq_len, dropout=dropout)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generation={'epoch_i': -1, 'score': -1.0803672075271606, 'encoder_text': '–ú–æ–∂–µ—Ç —Ç–∞–º –∫–∞–∫–∞—è-—Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –æ—Ç–≤–µ—Ç—ã —à–ª–µ—Ç ))<SEP>–ú–Ω–µ –≤–æ—Ç —Ç–æ–∂–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ) –∞ –µ—Å–ª–∏ –≤—Ä—É—á–Ω—É—é, —Ç–æ –¥—É–º–∞—é –∞–Ω–∫–µ—Ç—É —É–±—Ä–∞—Ç—å. –£ –º–µ–Ω—è —Ç–∞–º —Å –Ω–µ–π 110 —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–ª—É—á–∏–ª–æ—Å—åüòπ<SEP>110 —ç—Ç–æ –∂–µ—Å—Ç–∫–æ) —è –¥–∂–∏–ø–µ–≥–∞–º–∏ –ø—Ä–∏–ª–æ–∂–∏–ª –æ—Ç–¥–µ–ª—å–Ω–æ 9 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –ø–æ—Ç–æ–º —É–∂–µ –∫–æ–≥–¥–∞ –æ—Ç–ø—Ä–∞–≤–∏–ª –ø–æ–Ω—è–ª —á—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –¥–æ–∫–∏ –∑–∞–±—ã–ª, –ø–æ—Ç–æ–º —á–∏—Ç–∞—é –ª—é–¥–∏ –ø–¥—Ñ–∞–º–∏ —à–ª—é—Ç, –≤–æ—Ç –∏ –¥—É–º–∞—é —á–µ –¥–µ–ª–∞—Ç—å –∂–¥–∞—Ç—å –∏–ª–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ —Å–ª–∞—Ç—å<SEP>–ù—É –≤–æ—Ç –∏ —è –¥—É–º–∞—é. –¢–∞–º –Ω–∞–≤–µ—Ä–Ω–æ–µ —É—Å—Ç–∞–Ω—É—Ç –∏—Ö –ª–∏—Å—Ç–∞—Ç—åüòÇ –∏—Ö –Ω–∏—Ö 36 –≤—Ä–æ–¥–µ –∞–Ω–∫–µ—Ç–∞. –û—Å—Ç–∞–ª—å–Ω–æ–µ –¥–æ–∫–∏', 'predicted_text': '–î–∞, —è —Ç–∞–∫ –ø–æ–Ω–∏–º–∞—é, —á—Ç–æ –µ—Å–ª–∏ —è –Ω–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ–Ω—è–ª–∞, —Ç–æ —è –Ω–µ —É–≤–µ—Ä–µ–Ω–∞ —á—Ç–æ –æ–Ω–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω—ã, —á—Ç–æ –æ–Ω–∏ –º–æ–≥—É—Ç —Å–¥–µ–ª–∞—Ç—å –∞–Ω–∫–µ—Ç—É, –∞ –ø–æ—Ç–æ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ —Å–ª–∞—Ç—å –∏ —Ç.–ø. ü§∑ ‚ôÄÔ∏èü§∑ ‚ôÄÔ∏èü§∑ ‚ôÄÔ∏èü§∑ ÔøΩÔøΩ', 'target_text': '74 –¥–æ–∫–∞ –¥–∞ –æ–Ω–∏ –≤ —à–æ–∫–µ —Ç–∞–º'}\n",
      "Sample generation={'epoch_i': -1, 'score': -1.2391160726547241, 'encoder_text': '–ú–æ–∂–µ—Ç —Ç–∞–º –∫–∞–∫–∞—è-—Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –æ—Ç–≤–µ—Ç—ã —à–ª–µ—Ç ))<SEP>–ú–Ω–µ –≤–æ—Ç —Ç–æ–∂–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ) –∞ –µ—Å–ª–∏ –≤—Ä—É—á–Ω—É—é, —Ç–æ –¥—É–º–∞—é –∞–Ω–∫–µ—Ç—É —É–±—Ä–∞—Ç—å. –£ –º–µ–Ω—è —Ç–∞–º —Å –Ω–µ–π 110 —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–ª—É—á–∏–ª–æ—Å—åüòπ<SEP>110 —ç—Ç–æ –∂–µ—Å—Ç–∫–æ) —è –¥–∂–∏–ø–µ–≥–∞–º–∏ –ø—Ä–∏–ª–æ–∂–∏–ª –æ—Ç–¥–µ–ª—å–Ω–æ 9 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –ø–æ—Ç–æ–º —É–∂–µ –∫–æ–≥–¥–∞ –æ—Ç–ø—Ä–∞–≤–∏–ª –ø–æ–Ω—è–ª —á—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –¥–æ–∫–∏ –∑–∞–±—ã–ª, –ø–æ—Ç–æ–º —á–∏—Ç–∞—é –ª—é–¥–∏ –ø–¥—Ñ–∞–º–∏ —à–ª—é—Ç, –≤–æ—Ç –∏ –¥—É–º–∞—é —á–µ –¥–µ–ª–∞—Ç—å –∂–¥–∞—Ç—å –∏–ª–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ —Å–ª–∞—Ç—å<SEP>–ù—É –≤–æ—Ç –∏ —è –¥—É–º–∞—é. –¢–∞–º –Ω–∞–≤–µ—Ä–Ω–æ–µ —É—Å—Ç–∞–Ω—É—Ç –∏—Ö –ª–∏—Å—Ç–∞—Ç—åüòÇ –∏—Ö –Ω–∏—Ö 36 –≤—Ä–æ–¥–µ –∞–Ω–∫–µ—Ç–∞. –û—Å—Ç–∞–ª—å–Ω–æ–µ –¥–æ–∫–∏', 'predicted_text': '–°–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ –∑–∞ –æ—Ç–≤–µ—Ç!', 'target_text': '74 –¥–æ–∫–∞ –¥–∞ –æ–Ω–∏ –≤ —à–æ–∫–µ —Ç–∞–º'}\n",
      "Sample generation={'epoch_i': -1, 'score': -0.8571933507919312, 'encoder_text': '–ø–æ–¥–∫–∞–∂–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞: 1. –ø—Ä–∏—Ö–æ–¥–∏—Ç –ª–∏ —Å–º—Å —Å –∫–æ–¥–æ–º –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¢–ú? –º–Ω–µ –ø—Ä–∏—à–µ–ª —Ç—Ä–µ–∫–∏–Ω–≥ –Ω–æ–º–µ—Ä –±–µ–∑ —Å–º—Å. 2. –µ—Å–ª–∏ –ø–æ–¥–∞–≤–∞–ª–∏ –Ω–∞ 2 –ø–∞—Å–ø–æ—Ä—Ç–∞, —Ç–æ –¥–æ—Å—Ç–∞–≤–∫–∞ –±—É–¥–µ—Ç –ø–æ –æ–¥–Ω–æ–º–∫ —Ç—Ä–µ–∫–∏–Ω–≥—É –∏–ª–∏ –¥–≤—É–º—è —Ä–∞–∑–Ω—ã–º–∏ –¥–æ—Å—Ç–∞–≤–∫–∞–º–∏?<SEP>–¢—Ä–µ–∫–∏–Ω–≥ –ø–æ—Å—ã–ª–∞–µ—Ç –ø–æ—á—Ç–∞, –∞ —Å–º—Å –ú–í–î, –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –∫–æ–Ω–≤–µ—Ä—Ç–µ<SEP>–°–ø–∞—Å–∏–±–æ! –ê —Å–º—Å –∫–æ–≥–¥–∞ –ø—Ä–∏—Ö–æ–¥–∏—Ç?', 'predicted_text': '–ö–æ–≥–¥–∞ –ø—Ä–∏—Ö–æ–¥–∏—Ç —Å–º—Å, –ø—Ä–∏—Ö–æ–¥–∏—Ç —Å–º—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–∑, –∞ —Å–º—Å –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø–æ –ø–æ—á—Ç–µ, –∞ —Å–º—Å –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø–æ –ø–æ—á—Ç–µ, –∞ —Å–º—Å –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø–æ –ø–æ—á—Ç–µ, –∞ —Å–º—Å –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø–æ –ø–æ—á—Ç–µ, –∞ —Å–º—Å –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø–æ –ø–æ—á—Ç–µ', 'target_text': '–ö–æ–≥–¥–∞ —É –ø–∏—Å—å–º–∞ –∏–∑–º–µ–Ω–∏—Ç—Å—è —Å—Ç–∞—Ç—É—Å –Ω–∞ \"–≤—ã–¥–∞–Ω–æ –∞–¥—Ä–µ—Å–∞—Ç—É\"'}\n",
      "Sample generation={'epoch_i': -1, 'score': -0.9255996346473694, 'encoder_text': '–ø–æ–¥–∫–∞–∂–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞: 1. –ø—Ä–∏—Ö–æ–¥–∏—Ç –ª–∏ —Å–º—Å —Å –∫–æ–¥–æ–º –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¢–ú? –º–Ω–µ –ø—Ä–∏—à–µ–ª —Ç—Ä–µ–∫–∏–Ω–≥ –Ω–æ–º–µ—Ä –±–µ–∑ —Å–º—Å. 2. –µ—Å–ª–∏ –ø–æ–¥–∞–≤–∞–ª–∏ –Ω–∞ 2 –ø–∞—Å–ø–æ—Ä—Ç–∞, —Ç–æ –¥–æ—Å—Ç–∞–≤–∫–∞ –±—É–¥–µ—Ç –ø–æ –æ–¥–Ω–æ–º–∫ —Ç—Ä–µ–∫–∏–Ω–≥—É –∏–ª–∏ –¥–≤—É–º—è —Ä–∞–∑–Ω—ã–º–∏ –¥–æ—Å—Ç–∞–≤–∫–∞–º–∏?<SEP>–¢—Ä–µ–∫–∏–Ω–≥ –ø–æ—Å—ã–ª–∞–µ—Ç –ø–æ—á—Ç–∞, –∞ —Å–º—Å –ú–í–î, –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –∫–æ–Ω–≤–µ—Ä—Ç–µ<SEP>–°–ø–∞—Å–∏–±–æ! –ê —Å–º—Å –∫–æ–≥–¥–∞ –ø—Ä–∏—Ö–æ–¥–∏—Ç?', 'predicted_text': '–ö–æ–≥–¥–∞ –ø—Ä–∏—Ö–æ–¥–∏—Ç —Å–º—Å –Ω–∞ –ø–æ—á—Ç—É, –∞ —Å–º—Å –Ω–∞ –ø–æ—á—Ç—É –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø–æ –ø–æ—á—Ç–µ, –∞ —Å–º—Å –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø–æ –ø–æ—á—Ç–µ', 'target_text': '–ö–æ–≥–¥–∞ —É –ø–∏—Å—å–º–∞ –∏–∑–º–µ–Ω–∏—Ç—Å—è —Å—Ç–∞—Ç—É—Å –Ω–∞ \"–≤—ã–¥–∞–Ω–æ –∞–¥—Ä–µ—Å–∞—Ç—É\"'}\n",
      "Sample generation={'epoch_i': -1, 'score': -0.9939466714859009, 'encoder_text': '(–ê –≤—ã –Ω–∞—à–ª–∏ –∑–∞—á–µ–º –Ω—É–∂–Ω–æ gonogo –¥–∞—ë—Ç —Ç–µ–º, –∫—Ç–æ –Ω–µ –ø—Ä–µ—Ç–µ–Ω–¥—É–µ—Ç –Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –±–∏–ª–µ—Ç—ã? –ñ–∏–ª—å—ë –Ω–µ –¥–∞—é—Ç. –°—Ç—Ä–∞—Ö–æ–≤–∫—É –¥–∞—é—Ç –Ω–µ —Ñ–∞–∫—Ç —á—Ç–æ –≤ —Å–≤—è–∑–∏ —Å —ç—Ç–∏–º, –∏ –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ –æ–Ω–∞ –¥–µ–π—Å—Ç–≤—É–µ—Ç —Ç–æ–ª—å–∫–æ –¥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–∞, –∏ –≤–æ–æ–±—â–µ —Ñ–∏–≥ —Å –Ω–µ–π. –ß—Ç–æ –µ—â—ë?..)<SEP>–ú–æ–∂–Ω–æ —Å –∫–æ–Ω—Å—É–ª–æ–º –≤ –∞–µ—Ä–æ–ø–æ—Ä—Ç—É –Ω–µ –≤–µ—Å—Ç–∏ –±–µ—Å–µ–¥—ã - —è —Ç–∞–∫ –ø–æ–Ω—è–ª–∞<SEP>–ù–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –Ω–µ –≤–µ—Å—Ç–∏ –±–µ—Å–µ–¥—ã. –í—á–µ—Ä–∞ –≤ –∞—ç—Ä–æ–ø–æ—Ä—Ç—É –∫–æ–Ω—Å—É–ª –±—ã–ª —Å–µ—Ä—å–µ–∑–Ω—ã–π', 'predicted_text': '–î–∞, —è —Ç–∞–∫ –ø–æ–Ω—è–ª–∞, —Å–ø–∞—Å–∏–±–æ üôèüèª', 'target_text': '–ú–∞—Ä–∏–Ω–∞, —Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ'}\n",
      "Sample generation={'epoch_i': -1, 'score': -1.0583263635635376, 'encoder_text': '(–ê –≤—ã –Ω–∞—à–ª–∏ –∑–∞—á–µ–º –Ω—É–∂–Ω–æ gonogo –¥–∞—ë—Ç —Ç–µ–º, –∫—Ç–æ –Ω–µ –ø—Ä–µ—Ç–µ–Ω–¥—É–µ—Ç –Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –±–∏–ª–µ—Ç—ã? –ñ–∏–ª—å—ë –Ω–µ –¥–∞—é—Ç. –°—Ç—Ä–∞—Ö–æ–≤–∫—É –¥–∞—é—Ç –Ω–µ —Ñ–∞–∫—Ç —á—Ç–æ –≤ —Å–≤—è–∑–∏ —Å —ç—Ç–∏–º, –∏ –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ –æ–Ω–∞ –¥–µ–π—Å—Ç–≤—É–µ—Ç —Ç–æ–ª—å–∫–æ –¥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–∞, –∏ –≤–æ–æ–±—â–µ —Ñ–∏–≥ —Å –Ω–µ–π. –ß—Ç–æ –µ—â—ë?..)<SEP>–ú–æ–∂–Ω–æ —Å –∫–æ–Ω—Å—É–ª–æ–º –≤ –∞–µ—Ä–æ–ø–æ—Ä—Ç—É –Ω–µ –≤–µ—Å—Ç–∏ –±–µ—Å–µ–¥—ã - —è —Ç–∞–∫ –ø–æ–Ω—è–ª–∞<SEP>–ù–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –Ω–µ –≤–µ—Å—Ç–∏ –±–µ—Å–µ–¥—ã. –í—á–µ—Ä–∞ –≤ –∞—ç—Ä–æ–ø–æ—Ä—Ç—É –∫–æ–Ω—Å—É–ª –±—ã–ª —Å–µ—Ä—å–µ–∑–Ω—ã–π', 'predicted_text': '–î–∞, –≤–æ—Ç —è –∏ –ø–æ–Ω—è–ª–∞, —á—Ç–æ –µ—Å–ª–∏ —è –Ω–µ –≤–µ—Å—Ç–∏ –±–µ—Å–µ–¥—ã, —Ç–æ —è –Ω–µ –º–æ–≥—É –≤–µ—Å—Ç–∏ –±–µ—Å–µ–¥—ã - —è —Ç–∞–∫ –ø–æ–Ω–∏–º–∞—é, —á—Ç–æ –µ—Å–ª–∏ —è –Ω–µ —Ö–æ—á—É –≤–µ—Å—Ç–∏ –±–µ—Å–µ–¥—ã - —è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ–Ω–∏–º–∞—é, —á—Ç–æ —è –Ω–µ —Ö–æ—á—É –≤–µ—Å—Ç–∏ –±–µ—Å–µ–¥—ã - —è —Ç–∞–∫ –ø–æ–Ω–∏–º–∞—é, —á—Ç–æ —è –Ω–µ –º–æ–≥—É –≤–µ—Å—Ç–∏ –±–µ—Å–µ–¥—ã.', 'target_text': '–ú–∞—Ä–∏–Ω–∞, —Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ'}\n"
     ]
    }
   ],
   "source": [
    "sample_indices = np.random.randint(0, len(val_dataset), 3, dtype=np.int32)\n",
    "samples = []\n",
    "for sample_idx in sample_indices:\n",
    "    sample = val_dataset[sample_idx]\n",
    "    sample_text = decode_sample(sample, skip_special_tokens=True)\n",
    "    samples.append(\n",
    "        {\n",
    "            'encoder_ids': sample[0]['encoder_ids'],\n",
    "            'decoder_ids': sample[0]['decoder_ids'],\n",
    "            'target_ids': sample[1],\n",
    "            'encoder_text': sample_text['encoder_text'],\n",
    "            'target_text': sample_text['target_text'],\n",
    "        }\n",
    "    )\n",
    "\n",
    "def generate_sample(epoch_i, model, tensorboard: SummaryWriterHelper=None):\n",
    "    for sample in samples:\n",
    "        encoder_ids = sample['encoder_ids'].to(device)\n",
    "        decoder_ids = sample['decoder_ids'][:5].to(device)\n",
    "        seed_token_ids = { 'encoder_ids' : encoder_ids, 'decoder_ids' : decoder_ids }\n",
    "        hypothesis_list = model.generate(seed_token_ids=seed_token_ids, max_len=config.DECODER_SEQ_LEN, return_hypotheses_n=2, beamsize=5)\n",
    "        for score, hypothesis in hypothesis_list:\n",
    "            predicted_text = decode_tensor(hypothesis, skip_special_tokens=True)\n",
    "            sample_table = {\n",
    "                'epoch_i': epoch_i,\n",
    "                'score': score,\n",
    "                'encoder_text' : sample['encoder_text'],\n",
    "                'predicted_text' : predicted_text,\n",
    "                'target_text' : sample['target_text'],\n",
    "            }\n",
    "            print(f\"Sample generation={sample_table}\")\n",
    "            if tensorboard:\n",
    "                tensorboard.add_text(\"sample\", str(sample_table))\n",
    "generate_sample(-1, model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.BATCH_SIZE = 6\n",
    "\n",
    "def cross_entropy_loss(predict, target):\n",
    "    B, T, C = predict.shape\n",
    "    predict = predict.view(B*T, C)\n",
    "    target = target.view(B*T)\n",
    "    loss = F.cross_entropy(predict, target, ignore_index=-100)\n",
    "    return loss\n",
    "\n",
    "writer = SummaryWriterHelper()\n",
    "\n",
    "def scheduler(optim): return \\\n",
    "    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "best_val_loss, best_model = train_eval_loop(model=model,\n",
    "                                            device=device,\n",
    "                                            train_dataset=train_dataset,\n",
    "                                            val_dataset=val_dataset,\n",
    "                                            criterion=cross_entropy_loss,\n",
    "                                            lr=1e-4,\n",
    "                                            epoch_n=200,\n",
    "                                            batch_size=config.BATCH_SIZE,\n",
    "                                            l2_reg_alpha=1e-2,\n",
    "                                            lr_scheduler_ctor=scheduler,\n",
    "                                            shuffle_train=True,\n",
    "                                            tensorboard=writer,\n",
    "                                            on_epoch_cb=generate_sample\n",
    "                                            )\n",
    "print(\"saving best model...\")\n",
    "torch.save(best_model.state_dict(), 'data/model.pth')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'data/model.pth', map_location=device))\n",
    "best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> –î–∞, –≤–æ—Ç –∏ —è –¥—É–º–∞–ª, —á—Ç–æ —ç—Ç–æ –Ω–æ–≤–∞—è –≤–µ—Ä—Å–∏—è, –∞ —è –∏ –Ω–µ —É–≤–µ—Ä–µ–Ω–∞, —á—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –µ—â—ë –∏ –Ω–µ —Å–∏–ª—å–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–∞—è.</s>'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('–°—Ç—Ä–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ø–æ–ª—É—á–∏–ª–∞—Å—å', max_length= config.ENCODER_SEQ_LEN, truncation=True, padding='max_length',return_tensors='pt').to(device)\n",
    "output_ids = model.backbone.generate(\n",
    "                    inputs=input_ids, \n",
    "                    num_return_sequences=1,  \n",
    "                    num_beams=5, \n",
    "                    max_length=config.DECODER_SEQ_LEN,\n",
    "                    do_sample=True,\n",
    "                )\n",
    "tokenizer.decode(output_ids[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sharedenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
